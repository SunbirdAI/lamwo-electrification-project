{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f454f1e1-d6ba-40e4-8e3d-209bb1e80478",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f454f1e1-d6ba-40e4-8e3d-209bb1e80478",
    "outputId": "c8ecdb41-4d94-4812-9041-deff3956bdfd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.4.0-py3-none-any.whl (342 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.1/342.1 KB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /venv/main/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 KB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.0/462.0 KB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /venv/main/lib/python3.10/site-packages (from accelerate) (2.5.1+cu121)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 KB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: platformdirs in /venv/main/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Collecting click!=8.0.0,>=7.1\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 KB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /venv/main/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.22.0-py2.py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.8/325.8 KB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0\n",
      "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 KB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting pydantic<3,>=2.6\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 KB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /venv/main/lib/python3.10/site-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /venv/main/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 KB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.27.2\n",
      "  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: triton==3.1.0 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /venv/main/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Installing collected packages: threadpoolctl, smmap, setproctitle, sentry-sdk, scipy, safetensors, regex, pydantic-core, protobuf, joblib, docker-pycreds, click, annotated-types, scikit-learn, pydantic, gitdb, tokenizers, gitpython, wandb, transformers, accelerate\n",
      "Successfully installed accelerate-1.4.0 annotated-types-0.7.0 click-8.1.8 docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 joblib-1.4.2 protobuf-5.29.3 pydantic-2.10.6 pydantic-core-2.27.2 regex-2024.11.6 safetensors-0.5.2 scikit-learn-1.6.1 scipy-1.15.2 sentry-sdk-2.22.0 setproctitle-1.3.4 smmap-5.0.2 threadpoolctl-3.5.0 tokenizers-0.21.0 transformers-4.49.0 wandb-0.19.7\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate scikit-learn wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d324a18-c07a-4d82-a9d0-05757e960b98",
   "metadata": {
    "id": "8d324a18-c07a-4d82-a9d0-05757e960b98"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import torch\n",
    "import transformers\n",
    "import torchvision\n",
    "import accelerate\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import random_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import VisionDataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf81512-b3b5-4766-87d0-d3bca16fe706",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acf81512-b3b5-4766-87d0-d3bca16fe706",
    "outputId": "3deea050-9bb4-4592-dcea-2b05b55a0c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583daa5b-3ed4-4dcd-8b17-531573c4aa66",
   "metadata": {
    "id": "583daa5b-3ed4-4dcd-8b17-531573c4aa66"
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"GIZ_model_trainer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84930106-23b7-47b9-9be4-dc79df607831",
   "metadata": {
    "id": "84930106-23b7-47b9-9be4-dc79df607831"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8205f-b0b0-4af8-ab2e-a590f9943f03",
   "metadata": {
    "id": "fef8205f-b0b0-4af8-ab2e-a590f9943f03"
   },
   "source": [
    "Initialize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e19c4-dab0-4a2a-8338-575fdf2dbe20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195,
     "referenced_widgets": [
      "ee8fed2bdc0a4bcab2c3cbd536b65227",
      "62d2b46b16b74b608c8aafc72a2cf0e6",
      "e717efb7d50d48c9ae2fbdc5bdfc18fe",
      "c355fc1485514547b42a87078b731306",
      "a160a00bf3d44b39b54cbefba1b22e28",
      "6481ad9442d942ce834164c8f9ec6114",
      "e93b2269fff148e6b17faa3a473e4137",
      "137f8037a1c0409bb08cfcef0332fdcf",
      "10109ce0e33c44ae8488aa86bf0d39f4",
      "b9779e61078347ff9cc767330238fcac",
      "4aea069b1b2d4bf683523136ff6c1fc1",
      "8c82d9589d0e4d94bab68ee408410c57"
     ]
    },
    "id": "9b1e19c4-dab0-4a2a-8338-575fdf2dbe20",
    "outputId": "cf69e408-68c5-4f2b-dee3-209220261e25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c82d9589d0e4d94bab68ee408410c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GIZ_model_data.zip:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'GIZ_model_data.zip'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Download the file from Hugging Face Hub\n",
    "hf_hub_download(repo_id=\"Sunbird/GIZ-buildingsprediction-small\", repo_type=\"dataset\", filename=\"GIZ_model_data.zip\", local_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdecbe-fc31-4f0e-858d-97d4f1c9bc80",
   "metadata": {
    "id": "31bdecbe-fc31-4f0e-858d-97d4f1c9bc80"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "# Extract the zip file\n",
    "output_dir = \"./\"  # Specify where to extract\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(\"GIZ_model_data.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(output_dir)\n",
    "\n",
    "print(f\"Files extracted to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e320c2-14d7-455b-87d3-ba232c734844",
   "metadata": {
    "id": "84e320c2-14d7-455b-87d3-ba232c734844"
   },
   "outputs": [],
   "source": [
    "class CustomImageDataset(VisionDataset):\n",
    "    def __init__(self, root, geo_metadata_path, transform=None, target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Path to the directory containing the images and targets.\n",
    "            geo_metadata_path (str): Path to the pickle file containing geospatial metadata.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "            target_transform (callable, optional): Optional transform to be applied on the target.\n",
    "\n",
    "        Notes:\n",
    "            - The dataset is split into training and test sets.\n",
    "            - The training set consists of 80,000 locations, while the test set contains 20,000 locations.\n",
    "            - Geospatial data (latitude and longitude) is loaded from the provided pickle file.\n",
    "            - Future improvements could include filtering or grouping data based on geographic regions or using coordinates as input features.\n",
    "        \"\"\"\n",
    "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
    "        self.data = self._load_data()\n",
    "        self.geo_metadata = self._load_geo_metadata(geo_metadata_path)\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Recursively scans the directory structure and pairs image groups with their targets.\"\"\"\n",
    "        data = []\n",
    "        for dirpath, _, filenames in os.walk(self.root):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\"_0.jpg\"):\n",
    "                    base_name = filename.split(\"_0.jpg\")[0]\n",
    "                    images = [os.path.join(dirpath, f\"{base_name}_{i}.jpg\") for i in range(4)]  # Adjust if more than 4 images\n",
    "                    target_file = os.path.join(dirpath, f\"{base_name}_target.txt\")\n",
    "\n",
    "                    # Check for missing files\n",
    "                    missing_files = [img for img in images if not os.path.exists(img)]\n",
    "                    if missing_files:\n",
    "                        print(f\"Warning: Missing image files: {missing_files}\")\n",
    "                        continue\n",
    "\n",
    "                    if not os.path.exists(target_file):\n",
    "                        print(f\"Warning: Missing target file: {target_file}\")\n",
    "                        continue\n",
    "\n",
    "                    data.append((images, target_file, base_name))\n",
    "        return data\n",
    "\n",
    "    def _load_geo_metadata(self, geo_metadata_path):\n",
    "        \"\"\"Loads geospatial metadata from the pickle file.\"\"\"\n",
    "        try:\n",
    "            with open(geo_metadata_path, 'rb') as f:\n",
    "                geo_data = pickle.load(f)\n",
    "                return {str(idx): (lat, lon) for lat, lon, idx in geo_data}\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error loading geospatial metadata: {e}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            images, target_file, base_name = self.data[idx]\n",
    "\n",
    "            # Load and concatenate images into a single 12-channel tensor\n",
    "            channels = []\n",
    "            for img_path in images:\n",
    "                try:\n",
    "                    img = Image.open(img_path).convert(\"RGB\")  # Ensure 3-channel image\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "                img = transforms.ToTensor()(img)  # Convert image to tensor (3, H, W)\n",
    "                channels.append(img)\n",
    "\n",
    "            input_tensor = torch.cat(channels, dim=0)  # (12, H, W)\n",
    "\n",
    "            # Apply transforms (including normalization) to final 12-channel tensor\n",
    "            if self.transform:\n",
    "                input_tensor = self.transform(input_tensor)  # Apply normalization\n",
    "\n",
    "            # Load target\n",
    "            target_path = target_file\n",
    "            try:\n",
    "                with open(target_path, 'r') as f:\n",
    "                    target = float(f.read().strip())\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Error loading target file {target_path}: {e}\")\n",
    "\n",
    "            if self.target_transform:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "            # Retrieve geospatial metadata\n",
    "            geo_metadata = self.geo_metadata.get(base_name)\n",
    "            if geo_metadata is None:\n",
    "                raise RuntimeError(f\"Geospatial metadata not found for index {base_name}\")\n",
    "\n",
    "            latitude, longitude = geo_metadata\n",
    "\n",
    "            return {\n",
    "                \"x\": input_tensor,  # Normalized 12-channel tensor\n",
    "                \"labels\": torch.tensor(target, dtype=torch.float).unsqueeze(-1),\n",
    "                \"coords\": [latitude, longitude]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in __getitem__ for index {idx}: {e}\")\n",
    "            raise\n",
    "    def extra_repr(self):\n",
    "        return f\"Root: {self.root}, Number of samples: {len(self)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47e25cd9-f1e3-4e29-b5bb-8e2d21cda8e2",
   "metadata": {
    "id": "47e25cd9-f1e3-4e29-b5bb-8e2d21cda8e2"
   },
   "outputs": [],
   "source": [
    "# Initialize the dataset (No normalization yet) Data is converted to tensor within the initialization\n",
    "train_dataset = CustomImageDataset(\n",
    "    root=\"./content/exp_set_30_40\",\n",
    "    geo_metadata_path=\"./train_set_coords.pkl\",\n",
    "    transform=None\n",
    ")\n",
    "\n",
    "# Use DataLoader for batch processing\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ZFMKiDiB4Kj",
   "metadata": {
    "id": "8ZFMKiDiB4Kj"
   },
   "source": [
    "Compute Means and Stds and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Nfv2pvquBqd5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nfv2pvquBqd5",
    "outputId": "313e5c6b-9ae3-4a4c-deba-6250866bb878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean and std...\n",
      "Computed Mean: [0.1217270940542221, 0.11625519394874573, 0.11000549048185349, 0.011712568812072277, 0.011117534711956978, 0.6160487532615662, 0.5525528192520142, 0.1383216381072998, 0.1876230090856552, 0.07891623675823212, 0.30977097153663635, 0.1511278599500656]\n",
      "Computed Std: [0.05446215718984604, 0.03039601258933544, 0.03132794052362442, 0.07931023091077805, 0.07740972191095352, 0.04288787022233009, 0.05698736011981964, 0.21387967467308044, 0.14159177243709564, 0.17825061082839966, 0.2660183608531952, 0.13877056539058685]\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_std(loader):\n",
    "    \"\"\"\n",
    "    Computes mean and standard deviation for a dataset using a DataLoader.\n",
    "\n",
    "    Args:\n",
    "        loader (DataLoader): PyTorch DataLoader providing batches of images.\n",
    "\n",
    "    Returns:\n",
    "        mean (torch.Tensor): Mean per channel.\n",
    "        std (torch.Tensor): Standard deviation per channel.\n",
    "    \"\"\"\n",
    "    sum_channels = None\n",
    "    sum_squares_channels = None\n",
    "    num_pixels = 0\n",
    "\n",
    "    print(\"Computing mean and std...\")\n",
    "\n",
    "    for batch in loader:\n",
    "        images = batch[\"x\"]  # Extract images from dictionary\n",
    "\n",
    "        # Ensure images have correct shape: (batch, channels, height, width)\n",
    "        if images.dim() != 4:\n",
    "            raise ValueError(f\"Expected images of shape (batch, channels, height, width), got {images.shape}\")\n",
    "\n",
    "        batch_size, num_channels, height, width = images.shape\n",
    "\n",
    "        # Initialize accumulators\n",
    "        if sum_channels is None:\n",
    "            sum_channels = torch.zeros(num_channels, dtype=torch.float32)\n",
    "            sum_squares_channels = torch.zeros(num_channels, dtype=torch.float32)\n",
    "\n",
    "        # Update accumulators\n",
    "        sum_channels += images.sum(dim=[0, 2, 3])  # Sum over batch, height, width\n",
    "        sum_squares_channels += (images ** 2).sum(dim=[0, 2, 3])  # Sum of squares\n",
    "\n",
    "        # Update total pixel count\n",
    "        num_pixels += batch_size * height * width\n",
    "\n",
    "    # Compute final mean and std\n",
    "    mean = sum_channels / num_pixels\n",
    "    std = torch.sqrt(sum_squares_channels / num_pixels - mean ** 2)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Compute statistics\n",
    "mean, std = compute_mean_std(train_loader)\n",
    "\n",
    "# Print results for verification\n",
    "print(f\"Computed Mean: {mean.tolist()}\")\n",
    "print(f\"Computed Std: {std.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uQigO9yBB3u5",
   "metadata": {
    "id": "uQigO9yBB3u5"
   },
   "outputs": [],
   "source": [
    "# Define transformation with computed normalization\n",
    "final_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())  # Apply computed mean & std\n",
    "])\n",
    "\n",
    "# Reinitialize train dataset with normalization\n",
    "train_dataset = CustomImageDataset(\n",
    "    root=\"./content/exp_set_30_40\",\n",
    "    geo_metadata_path=\"./train_set_coords.pkl\",\n",
    "    transform=final_transform\n",
    ")\n",
    "\n",
    "# Reinitialize test dataset (apply SAME normalization values)\n",
    "test_dataset = CustomImageDataset(\n",
    "    root=\"./content/exp_test_set_10_11250\",\n",
    "    geo_metadata_path=\"./test_set_coords.pkl\",\n",
    "    transform=final_transform  # Use same mean/std from training\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5340517f-bd46-4540-9cbb-43bbc3ef2843",
   "metadata": {
    "id": "5340517f-bd46-4540-9cbb-43bbc3ef2843"
   },
   "source": [
    "Load and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cdbb853-a8b2-4edf-9cdc-8407c4ee31e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "0cdbb853-a8b2-4edf-9cdc-8407c4ee31e2",
    "outputId": "58ed662f-a69a-45d5-8076-0031d1693d87"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2750' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2750/5000 06:27 < 05:17, 7.09 it/s, Epoch 8/16]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.890700</td>\n",
       "      <td>1.041689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.820500</td>\n",
       "      <td>0.935882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.794500</td>\n",
       "      <td>0.961074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.702216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>0.655841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.728400</td>\n",
       "      <td>0.647074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.773800</td>\n",
       "      <td>0.682396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.847725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.726400</td>\n",
       "      <td>0.844478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.732400</td>\n",
       "      <td>0.677844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.708900</td>\n",
       "      <td>0.663584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2750, training_loss=0.7731716031161222, metrics={'train_runtime': 388.7383, 'train_samples_per_second': 411.588, 'train_steps_per_second': 12.862, 'total_flos': 0.0, 'train_loss': 0.7731716031161222, 'epoch': 8.78594249201278})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from torchvision import models, transforms\n",
    "from transformers import PretrainedConfig\n",
    "\n",
    "# Define model\n",
    "def get_modified_resnet50(num_channels=12):\n",
    "    model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    # Modify the first convolution layer\n",
    "    original_conv = model.conv1\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_channels,\n",
    "        original_conv.out_channels,\n",
    "        kernel_size=original_conv.kernel_size,\n",
    "        stride=original_conv.stride,\n",
    "        padding=original_conv.padding,\n",
    "        bias=original_conv.bias\n",
    "    )\n",
    "    # Initialize the new conv layer\n",
    "    nn.init.kaiming_normal_(model.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    # Modify the final fully connected layer for regression\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_features, 1)  # Assuming single regression output\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_modified_resnet50().to(device)\n",
    "\n",
    "# Add a dummy configuration if it doesn't already exist.\n",
    "if not hasattr(model, \"config\"):\n",
    "    model.config = PretrainedConfig()\n",
    "    # Set the _name_or_path attribute, which the Trainer expects.\n",
    "    model.config._name_or_path = \"GIZ-building-regression-model\"\n",
    "\n",
    "# create custom trainer with huber loss\n",
    "class RegressionTrainer(transformers.Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Pass standard Trainer args and kwargs to the base Trainer class\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Get the device of the model\n",
    "        # device = model.device\n",
    "\n",
    "        # Move inputs to the same device\n",
    "        images = inputs[\"x\"].to(device)  # Move images to GPU\n",
    "        labels = inputs[\"labels\"].to(device)  # Move labels to GPU\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Huber (SmoothL1) loss\n",
    "        loss_fct = nn.SmoothL1Loss(reduction='mean')\n",
    "\n",
    "        # Compute loss for each sample\n",
    "        loss = loss_fct(outputs, labels)  # Loss tensor of shape (batch, channels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "REPO_ID = \"Sunbird/GIZ-building-regression-model\"\n",
    "\n",
    "# Step 6: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    max_steps=5000,\n",
    "    learning_rate=1e-3,\n",
    "    label_names=[\"labels\"],\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    dataloader_num_workers=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"wandb\",\n",
    "    logging_steps=250,\n",
    "    eval_steps=250,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,  # Lower MSE is better\n",
    "    no_cuda=False,  # Ensure Trainer uses GPU if available\n",
    "    # Hub-related arguments:\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=REPO_ID,\n",
    "    hub_token=os.environ[\"HF_TOKEN\"]\n",
    ")\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=training_args.learning_rate, weight_decay=training_args.weight_decay)\n",
    "num_training_steps = training_args.max_steps\n",
    "num_warmup_steps = int(0.1 * num_training_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Define Early Stopping Callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.0)\n",
    "\n",
    "# Step 8: Define the Trainer\n",
    "trainer = RegressionTrainer(\n",
    "    model=model,  # The modified ResNet50 model for regression\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Step 9: Train the Model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fd23030-0454-4f63-a569-62c8f0bd7601",
   "metadata": {
    "id": "5fd23030-0454-4f63-a569-62c8f0bd7601",
    "outputId": "1b8d2452-1e93-46e2-aefe-080e69db7edd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Sunbird/GIZ-building-regression-model/commit/96a0ce97b6638c11a18d44b813a571aa31d8fc66', commit_message='End of training', commit_description='', oid='96a0ce97b6638c11a18d44b813a571aa31d8fc66', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Sunbird/GIZ-building-regression-model', endpoint='https://huggingface.co', repo_type='model', repo_id='Sunbird/GIZ-building-regression-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098d879-39d6-49fc-8a11-5e8709c1661c",
   "metadata": {
    "id": "2098d879-39d6-49fc-8a11-5e8709c1661c",
    "outputId": "616ec5f8-a086-4fb1-a744-f2265987c7d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: tensor([[0.0483]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Example: Create a dummy input with shape (batch_size=1, channels=12, height=224, width=224)\n",
    "dummy_input = torch.randn(1, 12, 224, 224).to(device)  # Ensure it's on the same device as your model\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Forward pass with no gradient calculation\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(\"Model output:\", output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10109ce0e33c44ae8488aa86bf0d39f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "137f8037a1c0409bb08cfcef0332fdcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aea069b1b2d4bf683523136ff6c1fc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62d2b46b16b74b608c8aafc72a2cf0e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6481ad9442d942ce834164c8f9ec6114",
      "placeholder": "​",
      "style": "IPY_MODEL_e93b2269fff148e6b17faa3a473e4137",
      "value": "GIZ_model_data.zip: 100%"
     }
    },
    "6481ad9442d942ce834164c8f9ec6114": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a160a00bf3d44b39b54cbefba1b22e28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9779e61078347ff9cc767330238fcac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c355fc1485514547b42a87078b731306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9779e61078347ff9cc767330238fcac",
      "placeholder": "​",
      "style": "IPY_MODEL_4aea069b1b2d4bf683523136ff6c1fc1",
      "value": " 1.07G/1.07G [00:25&lt;00:00, 42.3MB/s]"
     }
    },
    "e717efb7d50d48c9ae2fbdc5bdfc18fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_137f8037a1c0409bb08cfcef0332fdcf",
      "max": 1068595001,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_10109ce0e33c44ae8488aa86bf0d39f4",
      "value": 1068595001
     }
    },
    "e93b2269fff148e6b17faa3a473e4137": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee8fed2bdc0a4bcab2c3cbd536b65227": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62d2b46b16b74b608c8aafc72a2cf0e6",
       "IPY_MODEL_e717efb7d50d48c9ae2fbdc5bdfc18fe",
       "IPY_MODEL_c355fc1485514547b42a87078b731306"
      ],
      "layout": "IPY_MODEL_a160a00bf3d44b39b54cbefba1b22e28"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
